---
title: "Fast and Easy Aggregation of Multi-Type and Survey Data in R"
author: "Sebastian Krantz"
date: '2021-01-09'
slug: advanced-data-aggregation
categories: ["R"]
tags: ["R", "collapse", "weighted", "aggregation", "survey"]
---

[*collapse*](https://sebkrantz.github.io/collapse/) is a C/C++ based package to facilitate and speed up advanced statistical computations in R. One of the key objectives for creating it was to introduce in R a fast, consistent, and easy to use toolset for aggregating complex datasets. This post showcases this functionality by aggregating 3 quite different survey datasets I happened to have used recently for a project:    

* A births dataset from the 2016 Demographic and Health Survey for Uganda (used for child mortality estimates, available [*here*](https://dhsprogram.com/data/dataset/Uganda_Standard-DHS_2016.cfm?flag=0)).

* A dataset of poverty estimates from the Uganda National Household Survey 2016/17 (used to compute district level poverty indicators, not available for direct download, documented [*here*](<https://ubos.org/wp-content/uploads/publications/03_20182016_UNHS_FINAL_REPORT.pdf>)).

* The Uganda National Population and Housing Census 2014 (for district level population estimates and other data, available [*here*](<https://mepd.shinyapps.io/Macro-Data-Portal/>) under UBOS). 


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, fig.width = 8, fig.height = 5, 
                      out.width = '100%', cache = TRUE, message = FALSE, 
                      warning = FALSE, error = FALSE)

oldopts <- options(width = 101L)

library(haven)
library(magrittr)
library(collapse)

# Set path to the Uganda National Household Survey 2016/17
UNHS_path <- "C:/Users/Sebastian Krantz/Google Drive/6. Data and Data Systems/UNHS1617/UNHS2016_17"
# Set path to the Uganda National Population and Housing Census 2014
CENS_path <- "C:/Users/Sebastian Krantz/Google Drive/6. Data and Data Systems/UBOS 2014 Census"
# Set path to the Uganda Demographic and Health Survey 2016
DHS_path <- "C:/Users/Sebastian Krantz/Google Drive/6. Data and Data Systems/DHS2016"

```

First, the STATA files are imported using the *haven* library. Columns with only missing values are removed from the DHS dataset, encoded columns are converted to factor variables.

```{r}
library(haven)
library(magrittr)
library(collapse)

# Uganda Demographic and Health Survey 2016: Birth Recode
DHSBR <- paste0(DHS_path, "/Data/UGBR7BDT - Births Recode/UGBR7BFL.dta") %>%  
         read_dta %>% get_vars(fNobs(.) > 0L) %>% as_factor

# Uganda National Household Survey 2016/17: Poverty Estimates
UNHSPOV <- paste0(UNHS_path, "/Household/pov16_rev1.dta") %>% 
           read_dta %>% as_factor

# Uganda National Population and Housing Census 2014
CENS <- paste0(CENS_path, "/UBOS 2014 Census.dta") %>% read_dta 

```


```{r, echo=FALSE}
#
UNHS1 <- paste0(UNHS_path, "/Household/GSEC1.dta") %>% 
           read_dta %>% as_factor

UNHSPOV$finalwgt <- UNHS1$finalwgt[ckmatch(UNHSPOV$hhid, UNHS1$hhid)]

UNHSPOV %<>% colorder(hhid, finalwgt)

```
We start with aggregating the DHS dataset. This data has 786 variables, most of which are categorical:

```{r}
fdim(DHSBR)

table(vclasses(DHSBR))

```
We can obtain a detailed statistical summary of the data using `descr`. The output prints nicely to the console, but can also be converted to a data.frame.

```{r}
descr(DHSBR, table = FALSE) %>% as.data.frame %>% head(10)
```

<!-- The sample is based on a stratified two-stage cluster design. 696 Enumeration Areas (Clusters) were drawn from the sampling frame of the 2014 Census and a sample of households is drawn from an updated list of households within the cluster (on average a cluster had 130 households). -->
The DHS sample comprises 20,880 selected households and 18,506 women being interviewed. Of these women 13,745 had given birth and are recorded in this dataset. As the descriptive statistics above show, the first column gives the women-id (caseid), and the second column an integer id (bidx) for each of the born children. 

The aggregation task for this dataset shall simply be to aggregate over the children for each women. A first step to decide how this aggregation is to be done is to examine which variables vary by women i.e. contain child characteristics.
```{r}
# Tabulate child-variant variables
table(varying(DHSBR, ~ caseid))

# Examine the numeric child-variant variables
DHSBR %>% fgroup_by(caseid) %>% num_vars %>% 
  get_vars(varying(.)) %>% namlab

```
These are all variables that we would prefer to aggregate using the average, not the sum or extreme values. It is also noteworthy that the weights don't vary by child, but only by women, so weighted aggregation is actually not necessary in this case.
```{r}
# Renaming weights variable
setrename(DHSBR, v005 = weights)
# Confirm that it does not vary by child
varying(DHSBR, weights ~ caseid)
```
Thus aggregation in this case is very simple using the `collap()` function, which by default aggregates numeric columns using the mean, and categorical columns using the statistical mode (i.e. the most frequent value):

```{r}
# Aggregating, same as collap(DHSBR, ~ caseid, fmean, fmode), or collapv(1)
DHSBR_agg <- collap(DHSBR, ~ caseid) %>% fdroplevels

head(DHSBR_agg)

# Aggregating preserves column order and data types / classes + attributes
identical(namlab(DHSBR_agg, class = TRUE), 
          namlab(DHSBR, class = TRUE))
```

Apart from the simplicity and speed of this solution, `collap()` by default preserves the original column order (argument `keep.col.order = TRUE`) and all attributes of columns and the data frame itself. So we can truly speak of an aggregated / collapsed version of this dataset. Calling `fdroplevels` on the result is a likewise highly optimized and non-destructive solution to dropping any redundant factor levels from any of the 696 aggregated factor variables. 

Let us now consider the poverty estimates dataset: 

```{r}
fdim(UNHSPOV)

table(vclasses(UNHSPOV))

descr(UNHSPOV, table = FALSE) %>% as.data.frame %>% head(10)
```
Using the `qsu()` function, we can also summarize the variation in two of the key variables between district averages and within districts, separated for rural and urban areas. This can give us an idea of the variation in poverty levels we are erasing by aggregating this data to the district level. 
```{r}
qsu(UNHSPOV, fexp30 + welfare ~ urban, ~ district, ~ finalwgt, 
    vlabels = TRUE)[,"SD",,] # Showing only the standard deviation (SD)
```
The variance breakdown shows that apart from rural welfare, most of the variation in food expenditure and welfare levels is between district averages rather than within districts. We can again examine the numeric variables: 
```{r}
UNHSPOV %>% num_vars %>% namlab
```
These are also all variables that we would aggregate using a measure of central tendency. The categorical variables are mostly identifiers and also some categorical versions of welfare variables (welfare quintiles), which can all sensibly be aggregated using the statistical mode:
```{r}
UNHSPOV %>% cat_vars %>% namlab
```

Below we aggregate this dataset, applying the weighted median to numeric data and the weighted mode (default) to categorical data, this time using `collapg` which is a wrapper around `collap` operating on grouped data frames / tibbles. 
```{r}
# Weighted aggregation by district, after removing household id and enumeration area
UNHSPOV %>% 
  fselect(-hhid, -ea) %>% 
  fgroup_by(district) %>% 
  collapg(fmedian, w = finalwgt) %>%
  fdroplevels %>% 
  head
```
Note in the result above that the weighting variable is also aggregated. The default is `wFUN = fsum` so the weights in each group are summed. 

At last let's consider the census dataset. On first sight it is a bit simpler than the other two, consisting of 5 character identifiers from the macro-region to the parish level, followed by 270 numeric variables.
```{r}
fdim(CENS)

table(vclasses(CENS))

```
The specialty of this data is however that some variables are recorded in population totals, and some in percentage terms. 

```{r}
descr(CENS, table = FALSE) %>% as.data.frame %>% head(15)
```

The population counts are easily aggregated by simply computing a sum, but variables providing percentages of the population need to be aggregated using a weighted mean, where the total population serves as the weighting variable. This shows the percentage change variables:

```{r}
# gvr is a shorthand for get_vars(..., regex = TRUE)
gvr(CENS, "_P$") %>% namlab %>% head(10)

# Making sure all of these variables are indeed on a percentage scale
range(fmax(gvr(CENS, "_P$")))

```

To aggregate this data with `collap`, we need to supply the names or indices of both percentage and non-percentage variables together with the corresponding aggregator functions in a list passed to the `custom` argument. Weights are passed to the `w` argument. A specialty here is that we are using `fsum_uw` instead of `fsum`. The postfix `_uw` prevents the weights from being passed to `fsum`, which would otherwise calculate a survey total (i.e. a weighted sum) instead of a simple summation.

```{r}
perc_vars <- gvr(CENS, "_P$", return = "indices")
pop_vars <- setdiff(num_vars(CENS, "indices"), perc_vars)

collap(CENS, ~ Region + District, w = ~ POP,
       custom = list(fmean = perc_vars, fsum_uw = pop_vars), 
       keep.w = FALSE) %>% head
```

Also with the custom argument, the columns are by default (`keep.col.order = TRUE`) rearranged into the order in which they occur. Here we additionally use `keep.w = FALSE`, because the variable `POP` is both used as the weighting variable but also contained in `pop_vars`, and we don't want to have it twice in the output. 


Since we are only aggregating numeric data, we may compare the computation speed with a matching *data.table* expression^[Which does however not maintain the original column order.]: 

```{r}
library(microbenchmark)
library(data.table)
setDT(CENS)

microbenchmark(
  data.table = cbind(CENS[, lapply(.SD, weighted.mean, POP), by = .(Region, District), .SDcols = perc_vars], 
                     CENS[, lapply(.SD, sum), by = .(Region, District), .SDcols = pop_vars][, -(1:2)]), 
  collapse = collap(CENS, ~ Region + District, w = ~ POP,
                     custom = list(fmean = perc_vars, fsum_uw = pop_vars), 
                     keep.w = FALSE)
)

```

