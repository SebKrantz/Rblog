---
title: 'collapse and the fastverse: Reflections on the Past, Present and Future'
subtitle: 'With Examples from Geospatial Data Science'
author: Package Build
date: '2023-04-09'
slug: collapse-and-the-fastverse-reflecting-the-past-present-and-future
categories: ["R"]
tags: ["R", "collapse", "fastverse"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, fig.width = 8, fig.height = 5, out.width = '100%', 
                      cache = TRUE, message = FALSE, error = FALSE, warning = FALSE)

oldopts <- options(width = 200L)
```

Last week [*collapse*](https://sebkrantz.github.io/collapse/) reached 1M downloads off CRAN. This is a milestone that was largely unforeseen for a package that started 4 years ago as a collection of functions intended to ease the R life of an economics master student. Today, *collapse* provides cutting-edge performance in many areas of statistical computing and data manipulation, and a breadth of statistical algorithms that can meet applied economists' or statisticians' demands on a programming environment like R. It is also the only programming framework in R that is effectively class-agnostic. Version 1.9.5 just released to CRAN this week, is also the first version that includes Single Instruction Multiple Data (SIMD) instructions for a limited set of operations. The future will see more efforts to take advantage of the capabilities of modern processors.

Meanwhile, the [*fastverse*](https://fastverse.github.io/fastverse/) - a lightweight collection of C/C++-based R packages for statistical computing and data manipulation - is becoming more popular as an alternative to the [*tidyverse*](https://www.tidyverse.org/) for data analysis and as backends to statistical packages developed for R - a trend that is needed.

It is on this positive occasion that I decided it was the right time to provide you with a personal note, or rather, some reflections, regarding the history, present state, and the future of *collapse* and the *fastverse*.

# The Past

*collapse* started in 2019 as a small package with only two functions: `collap()` - intended to facilitate the aggregation of mixed-type data in R, and `qsu()` - intended to facilitate summarizing panel data in R. Both were inspired by [*STATA's*](https://www.stata.com/) `collapse` and `(xt)summarize` commands, and implemented with [*data.table*](https://rdatatable.gitlab.io/data.table/) as a backend. The package - called *collapse* alluding to the STATA command - would probably have stayed this way, had not unforeseen events affected my career plans.

Having completed a master's in international economics in summer 2019, I was preparing for a two-year posting as an ODI Fellow in the Central Bank of Papua New Guinea in fall. However, things did not work out, I had difficulties getting a working visa and there were coordination issues with the Bank, so ODI decided to offer me a posting in the Ugandan Ministry of Finance, starting in January 2020. This gave me 4 months, September-December 2019, during which I ended up writing a new backend for *collapse* - in C++.

While *collapse* with *data.table* backed was never particularly slow, some of the underlying metaprogramming seemed arcane, especially because I wanted to utilize *data.table*'s GeForce optimizations which require the aggregation function to be recognizable in the call for *data.table* to internally replace it with an optimized version. But there were also statistical limitations. As an economist, I often employed sampling or trade weights in statistics, and in this respect, R was quite limited. There was also no straightforward way to aggregate categorical data, using, as I would have it, a (weighted) statistical mode function. I also felt R was lacking basic things in the time series domain - evidenced by the lengths I went to handle (irregular) trade panels. Finally, I felt limited by the division of software development around different classes in R. I found *data.table* useful for analytics, but the class too complex to behave in predictable ways. Thus I often ended up converting back to 'data.frame' or 'tibble' to use functions from a different package. Sometimes it would also have been practical to simply keep data as a vector or matrix - in linear-algebra-heavy programs - but I needed *data.table* to do something 'by groups'. So in short, my workflow in R employed frequent object conversions, faced statistical limitations, and, in the case of early *collapse*'s *data.table* backend, also involved tedious metaprogramming.

The will for change pushed me to practically rethink the way statistics could be done in R. It required a framework that encompassed increased statistical complexity, including advanced statistical algorithms like (weighted) medians, quantiles, modes, support for (irregular) time series and panels etc., and enabling these operations to be vectored efficiently across many groups and columns without a limiting syntax that would again encourage metaprogramming. The framework would also need to be class-agnostic/support multiple R objects and classes, to easily integrate with different frameworks and reduce the need for object conversions. These considerations led to the creation of a comprehensive set of S3 generic grouped and weighted [*Fast Statistical Functions*](https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html) for vectors matrices and data.frame-like objects, initially programmed fully in C++. The functions natively supported R factors for grouping. To facilitate programming further, I created multivariate [grouping ('GRP') objects](https://sebkrantz.github.io/collapse/reference/GRP.html) that could be used to perform multiple statistical operations across the same groups without grouping overhead. With this backend and hand, it was easy to reimplement [`collap()`](https://sebkrantz.github.io/collapse/reference/collap.html)^[[`qsu()`](https://sebkrantz.github.io/collapse/reference/qsu.html) was implemented fully in C++.], and also provide a whole array of other useful functions, including dplyr-like functions like `fgroup_by()`, and [time series functions](https://sebkrantz.github.io/collapse/reference/time-series-panel-series.html) that could be used ad-hoc but also supported [*plm*'s](https://cran.r-project.org/package=plm) indexed 'pseries' and 'pdata.frame' classes. *collapse* 1.0.0, released to CRAN on 19th March 2020 (me sitting in the Ugandan finance ministry) was already a substantial piece of statistical software offering cutting-edge performance (see the benchmarks in the [*introductory blog post*](https://sebkrantz.github.io/Rblog/2020/08/31/welcome-to-collapse/)).

To then cut a long story short, in the coming 3 years *collapse* became better, broader, and faster in multiple iterations. Additional speed came especially from rewriting central parts of the package in C - reimplementing some core algorithms in C rather than relying on the [*C++ standard library*](https://en.cppreference.com/w/cpp/algorithm) or [*Rcpp sugar*](https://dirk.eddelbuettel.com/code/rcpp/Rcpp-sugar.pdf) - as well as introducing [data transformation by reference](https://sebkrantz.github.io/collapse/reference/TRA.html) and [OpenMP multithreading](https://sebkrantz.github.io/collapse/reference/collapse-options.html). For example, [`fmode()`](https://sebkrantz.github.io/collapse/reference/fmode.html), rewritten from C++ to C for v1.8.0 (May 2022), became about 3x faster in serial mode (grouped execution), with additional gains through multithreading across groups. Other noteworthy functionality was a modern reimplementation of 'pseries' and 'pdata.frame', through ['indexed_frame' and 'indexed_series' classes](https://sebkrantz.github.io/collapse/reference/indexing.html), fully fledged [`fsummarise()`](https://sebkrantz.github.io/collapse/reference/fsummarise.html), [`fmutate()`](https://sebkrantz.github.io/collapse/reference/ftransform.html) and [`across()`](https://sebkrantz.github.io/collapse/reference/across.html) functions enabling *tidyverse*-like programming with vectorization for *Fast Statistical Functions* in the backend, a set of functions facilitating [memory efficient R programming](https://sebkrantz.github.io/collapse/reference/efficient-programming.html) and low-cost [data object conversions](https://sebkrantz.github.io/collapse/reference/quick-conversion.html), functions to effectively deal with [(nested) lists of data objects](https://sebkrantz.github.io/collapse/reference/list-processing.html) - such as unlisting to data frame with [`unlist2d()`](https://sebkrantz.github.io/collapse/reference/unlist2d.html), and additional [descriptive statistical tools](https://sebkrantz.github.io/collapse/reference/summary-statistics.html) like [`qtab()`](https://sebkrantz.github.io/collapse/reference/qtab.html) and [`descr()`](https://sebkrantz.github.io/collapse/reference/descr.html). Particularly 2022 has seen two major updates: v1.7 and v1.8, and the bulk of development for 1.9 - released in January 2023. <!-- These updates significantly improved the functionality and performance of *collapse*, making it one of the most powerful statistical libraries available in any software environment. --> In improving *collapse*, I always took inspiration from other packages, most notably *data.table*, *kit*, *dplyr*, *fixest*, and R itself, to which I am highly indebted. The presentation of *collapse* at [UseR 2022](https://sebkrantz.github.io/collapse/index.html#presentation-at-user-2022) in June 2022 marks another milestone of its establishment in the R community.

While using R and improving *collapse*, I became increasingly aware that I was not alone in the pursuit of making R faster and statistically more powerful. Apart from well-established packages like *data.table*, *matrixStats*, and *fst*, I noticed and started using many smaller packages like *kit*, *roll*, *stringfish*, *qs*, *Rfast*, *coop*, *fastmap*, *fasttime*, *rrapply* etc. aimed at improving particular aspects of R in a statistical or computational sense, often offering clean C or C++ implementations with few R-level dependencies. I saw a pattern of common traits and development efforts that were largely complimentary and needed encouragement. My impression at the time - largely unaltered today - was that such efforts were ignored by large parts of the R user community. One reason is of course the lack of visibility and coordination, compared to institutional stakeholders like Rstudio and H2O backing the *tidyverse* and *data.table*. Another consideration, it seemed to me, was that the *tidyverse* is particularly popular simply because there exists an R package and website called *tidyverse* which loads a set of packages that work well together, and thus alleviates users of the burden of searching CRAN and choosing their own collection of data manipulation packages.

Thus I decided in early 2021 to also create a meta package and [GitHub repo](https://github.com/fastverse/fastverse) called [*fastverse*](https://fastverse.github.io/fastverse/) and use it to promote high-performance R packages with few dependencies. The first version 0.1.6 made it to CRAN in August 2021, attaching 6 core packages (*data.table*, *collapse*, *matrixStats*, *kit*, *fst* and *magrittr*), and allowing easy extension with additional packages using the `fastverse_extend()` function. With 7 total dependencies instead of 80, it was a considerably more lightweight and computationally powerful alternative to the *tidyverse*. The [README](https://github.com/fastverse/fastverse#suggested-extensions) of the GitHub repository has grown largely due to suggestions from the community and now lists many of the highest performing and (mostly) lightweight R packages. Over time I also introduced more useful functionality into the *fastverse* package, such as the ability to configure the environment and set of packages included using a [`.fastverse`](https://fastverse.github.io/fastverse/articles/fastverse_intro.html#custom-fastverse-configurations-for-projects) file, an option to [install missing packages on the fly](https://fastverse.github.io/fastverse/reference/fastverse_extend.html), and the [`fastverse_child()`](https://fastverse.github.io/fastverse/reference/fastverse_child.html) function to create wholly separate package verses. Observing my own frequent usage of *data.table*, *collapse*, *kit*, and *magrittr* in combination, I did a poll on Twitter in Fall 2022 suggesting the removal of *matrixStats* and *fst* from the core set of packages - which as accepted and implemented from v0.3.0 (November 2022). The *fastverse* package has thus become an extremely lightweight, customizable, and fast *tidyverse* alternative.

<!-- . The proposal was accepted, thus since v0.3.0 (November 2022), the core *fastverse* only consists of *data.table*, *collapse*, *kit* and *magrittr*, depends only on *Rcpp*, and offers several possibilities of customization and extension with additional packages. It has become an extremely lightweight, flexible and fast *tidyverse* alternative. -->

# The Present

Today, both *collapse* and *fastverse* are well established in a part of the R community closer to econometrics and high-performance statistics. A growing number of econometric packages benefit from *collapse* as a computational backend, most notably the well-known [*plm*](https://cran.r-project.org/package=plm) package - which experienced order-of-magnitude performance gains. I am also developing [*dfms*](<https://github.com/SebKrantz/dfms>) (first CRAN release October 2022), demonstrating that very efficient estimation of Dynamic Factor Models is possible in R combining *collapse* and *RcppArmadillo*. *collapse* is also powering various shiny apps across the web. I ended up creating a *collapse*-powered public [macroeconomic data portal](https://mepd.finance.go.ug/apps/macro-data-portal/) for Uganda, and later, at the Kiel Institute for the World Economy, for [Africa at large](https://africamonitor.ifw-kiel.de/).

So *collapse* has made it into production in my own work and the work of others. Core benefits in my experience are that it is lightweight to install on a server, has very low baseline function execution speeds (of a few microseconds instead of milliseconds with most other frameworks) making for speedy reaction times, scales very well to large data, and supports multiple R objects and modes of programming - reducing the need for metaprogramming. Since my own work and the work of others depends on it, API stability has always been important. *collapse* has not seen any major API changes in updates v1.7-v1.9, and currently no further API changes are planned. This lightweight and robust nature - characteristic of all core *fastverse* packages esp. *data.table* - stands in contrast to *dplyr*, who's core API involving `summarise()`, `mutate()` and `across()` keeps changing to an extent that at some point in 2022 I removed unit tests of `fsummarise()` and `fmutate()` against the *dplyr* versions from CRAN.

Apart from development, it has also been very fun using the *fastverse* in the wild for some research projects. Lately, I've been working a lot with geospatial data, where the *fastverse* has enabled numerous interesting applications. 

For example, I was interested in how the area of OSM buildings needs to be scaled using a power weight to correlate optimally with nightlights luminosity within a million cells of populated places in Sub-Saharan Africa. Having extracted around 12 million buildings from OSM, I programmed the following objective function and optimized it for power weights between 0.0001 and 5.

```{r}
library(fastverse)       
library(microbenchmark)

a <- abs(rnorm(12e6, 100, 100))            # Think of this as building areas in m^2
g <- GRP(sample.int(1e6, 12e6, TRUE))      # Think of this as grid cells
y <- fsum(a^1.5, g, use.g.names = FALSE) + # Think of this as nightlights 
     rnorm(g$N.groups, sd = 10000)  
length(y)

# Objective function
cor_ay <- function(w, a, y) {
  aw_agg = fsum(a^w, g, use.g.names = FALSE, na.rm = FALSE)
  cor(aw_agg, y) 
}

# Checking the speed of the objective
microbenchmark(cor_ay(2, a, y))

# Now the optimization
system.time(res <- optimise(cor_ay, c(0.0001, 5), a, y, maximum = TRUE))
res
```

The speed of the objective due to `GRP()` and `fsum()`^[Actually what is taking most time here is raising 12 million data points to a fractional power.] allowed further subdivision of buildings into different classes, and experimentation with finer spatial resolutions.

Another recent application involved finding the 100 nearest neighbors for each of around 100,000 cells (rows) in a rich geospatial dataset with about 50 variables (columns), and estimating a simple proximity-weighted linear regression of an outcome of interest `y` on a variable of interest `z`. Since computing a distance matrix on 100,000 rows up-front is infeasible memory-wise, I needed to go row-by-row. Here functions `dapply()`, `fdist()` and `flm()` from *collapse*, and `topn()` from *kit* became very handy.

```{r}
# Generating the data
X <- rnorm(5e6)       # 100,000 x 50 matrix of characteristics
dim(X) <- c(1e5, 50)
z <- rnorm(1e5)       # Characteristic of interest
y <- z + rnorm(1e5)   # Outcome of interest
Xm <- t(forwardsolve(t(chol(cov(X))), t(X)))    # Transform X to compute Mahalanobis distance (takes account of correlations)

# Coefficients for a single row
coef_i <- function(row_i) {
    mdist = fdist(Xm, row_i, nthreads = 2L)                           # Mahalanobis distance
    best_idx = topn(mdist, 101L, hasna = FALSE, decreasing = FALSE)   # 100 closest points
    best_idx = best_idx[mdist[best_idx] > 0]                          # Removing the point itself (mdist = 0)
    weights = 1 / mdist[best_idx]                                     # Distance weights
    flm(y[best_idx], z[best_idx], weights, add.icpt = TRUE)           # Weighted lm coefficients
}

# Benchmarking a single execution
microbenchmark(coef_i(Xm[1L, ]))

# Compute coefficients for all rows
system.time(coefs <- dapply(Xm, coef_i, MARGIN = 1))
head(coefs, 3)
```

Due to the efficiency of `fdist()` and `topn()`, a single call to the function takes around 1.2 milliseconds on the M1, giving a total execution time of around 120 seconds for 100,000 iterations of the program - one for each row of `Xm`.

A final recent application involved creating geospatial GINI coefficients for South Africa using remotely sensed population and nightlights data. Since population data from [WorldPop](https://hub.worldpop.org/geodata/listing?id=75) and Nightlights from [Google Earth Engine](https://developers.google.com/earth-engine/datasets/catalog/NOAA_VIIRS_DNB_MONTHLY_V1_VCMSLCFG) are easily obtained from the web, I reproduce the exercise here in full.

``` r
# Downloading 1km2 UN-Adjusted population data for South Africa from WorldPop
pop_year <- function(y) sprintf("https://data.worldpop.org/GIS/Population/Global_2000_2020_1km_UNadj/%i/ZAF/zaf_ppp_%i_1km_Aggregated_UNadj.tif", y, y)
for (y in 2014:2020) download.file(pop_year(y), mode = "wb", destfile = sprintf("data/WPOP_SA_1km_UNadj/zaf_ppp_%i_1km_Aggregated_UNadj.tif", y))
```

VIIRS Nightlights are available on [Google Earth Engine](https://developers.google.com/earth-engine/datasets/catalog/NOAA_VIIRS_DNB_MONTHLY_V1_VCMSLCFG) on a monthly basis from 2014 to 2022. I extracted annual median composites for South Africa using instructions found [here](https://worldbank.github.io/OpenNightLights/tutorials/mod3_6_making_VIIRS_annual_composites.html) and saved them to my [google drive](https://drive.google.com/drive/folders/18xI75APNFkUx4pcTfFdX8Orm36lcLzva?usp=share_link)[^1].

[^1]: You can download the composites for South Africa from my drive. It actually took me a while to figure out how to properly extract the images from Earth Engine. You may find [my answer](https://stackoverflow.com/questions/75822877/exporting-tif-images-from-google-earth-engine-to-google-drive-minimal-example-i) here helpful if you want to export images for other countries.

```{r, include=FALSE}
pop_path <- "~/Documents/IFW Kiel/Stellenbosh/Geospatial-Inequality/data/WPOP_SA_1km_UNadj"
nl_annual_path <- "~/Documents/IFW Kiel/Stellenbosh/Geospatial-Inequality/data/south_africa_viirs_dnb_nightlights_v1_vcmslcfg_annual_median_composite"
# options(datatable.print.topn = 2)
```

```{r}
# Reading population files using terra and creating a single data.table
pop_data <- list.files(pop_path) %>% 
  set_names(substr(., 9, 12)) %>% 
  lapply(function(x) paste0(pop_path, "/", x) %>% 
           terra::rast() %>% 
           terra::as.data.frame(xy = TRUE) %>% 
           set_names(c("lon", "lat", "pop"))) %>% 
  unlist2d("year", id.factor = TRUE, DT = TRUE) %>% 
  fmutate(year = as.integer(levels(year))[year]) %T>% print(2)

# Same for nightlights
nl_data <- list.files(nl_annual_path) %>% 
  set_names(substr(., 1, 4)) %>% 
  lapply(function(x) paste0(nl_annual_path, "/", x) %>% 
           terra::rast() %>% 
           terra::as.data.frame(xy = TRUE) %>% 
           fsubset(avg_rad %!=% -9999)) %>%  # Values outside land area of SA, coded using a mask in GEE
  unlist2d("year", id.factor = TRUE, DT = TRUE) %>%
  frename(x = lon, y = lat) %>% 
  fmutate(year = as.integer(levels(year))[year]) %T>% print(2)
```

Since nightlights are available up to 2022, but population only up to 2020, I did a crude cell-level population forecast for 2021 and 2022 based on 1.6 million linear models of cell-level population between 2014 and 2020.

```{r}
# Unset unneeded options for greater efficiency
set_collapse(na.rm = FALSE, sort = FALSE)
system.time({
# Forecasting population in 1.6 million grid cells based on linear regression
pop_forecast <- pop_data %>% 
  fgroup_by(lat, lon) %>% 
  fmutate(dm_year = fwithin(year)) %>% 
  fsummarise(pop_2020 = flast(pop),
             beta = fsum(pop, dm_year) %/=% fsum(dm_year^2)) %>% 
  fmutate(pop_2021 = pop_2020 + beta, 
          pop_2022 = pop_2021 + beta, 
          beta = NULL)
})

head(pop_forecast)
```

The above expression is an optimized version of univariate linear regression: `beta = cov(pop, year)/var(year) = sum(pop * dm_year) / sum(dm_year^2)`, where `dm_year = year - mean(year)`, that is fully vectorized across 1.6 million groups. Two further tricks are applied here: `fsum()` has an argument for sampling weights, which I utilize here instead of writing `fsum(pop * dm_year)`, which would require materializing a vector `pop * dm_year` before summing. The division by reference (`%/=%`) saves another unneeded copy. The expression could also have been written in one line as `fsummarise(beta = fsum(pop, W(year)) %/=% fsum(W(year)^2))`, given that 3/4 of the computation time here is actually spent on grouping 11.4 million records by `lat` and `lon`. <!-- This indicates just how fast vectorized operations with *collapse* are. -->

```{r}
# Appending population data with cell-level forecasts for 2021 and 2022
pop_data_forecast <- rbind(pop_data,
  pop_forecast %>% fselect(-pop_2020) %>% rm_stub("pop_") %>% 
  melt(1:2, variable.name = "year", value.name = "pop") %>% 
  fmutate(year = as.integer(levels(year))[year]) %>% 
  colorder(year))
```

As you may have noticed, the nightlights data has a higher resolution of around 464m than the population data at 1km resolution. To match the two datasets, I use a function that transforms the coordinates to a rectilinear grid of a certain size in km, using an Approximation to the [Haversine Formula](https://en.wikipedia.org/wiki/Haversine_formula) which rescales longitude coordinates based on the latitude coordinate (to have them approximately represent distance as at the equator). The coordinates are then divided by the grid size in km transformed to degrees at the equator, and the modulus from this division is removed. Afterward, half of the grid size is added again, reflecting the grid centroids. Finally, longitudes are rescaled back to their original extent using the same scale factor.

```{r}
# Transform coordinates to cell centroids of a rectilinear square grid of a certain size in kms
round_to_kms_fast <- function(lon, lat, km, round = TRUE, digits = 6) {
  degrees = km / (40075.017 / 360)             # Gets the degree-distance of the kms at the equator
  if(round) div = round(degrees, digits)       # Round to precision
  res_lat = TRA(lat, div, "-%%") %+=% (div/2)  # This transforms the data to the grid centroid
  scale_lat = cos(res_lat * pi/180)            # Approx. scale factor based on grid centroid
  res_lon = setTRA(lon * scale_lat, div, "-%%") %+=% (div/2) %/=% scale_lat  
  return(list(lon = res_lon, lat = res_lat))
}
```

The virtue of this approach, while appearing crude and not fully respecting the spherical earth model, is that it allows arbitrary grid sizes and transforms coordinates from different datasets in the same way. To determine the grid size, I take the largest 2-digit grid size that keeps the population cells unique, i.e. that largest number such that:

```{r}
pop_data %>% ftransform(round_to_kms_fast(lon, lat, 0.63)) %>% 
  fselect(year, lat, lon) %>% any_duplicated()
```

It turns out that 0.63km is the ideal grid size. I apply this to both datasets and merge them, aggregating nightlights using the mean^[Not the sum, as there could be a differing amount of nightlights observations for each cell.].

```{r}
system.time({
nl_pop_data <- pop_data_forecast %>% 
   ftransform(round_to_kms_fast(lon, lat, 0.63)) %>% 
   merge(nl_data %>% ftransform(round_to_kms_fast(lon, lat, 0.63)) %>% 
         fgroup_by(year, lat, lon) %>% fmean(), 
         by = .c(year, lat, lon))
})
head(nl_pop_data, 2)
```

Given the matched data, I define a function to compute the weighted GINI coefficient and an unweighted version for comparison.

```{r}
# Taken from Wikipedia: with small-sample correction
gini_wiki <- function(x) 1 + 2/(length(x)-1) * (sum(seq_along(x)*sort(x)) / sum(x) - length(x))

# No small-sample correction
gini_noss <- function(x) 2/length(x) * sum(seq_along(x)*sort(x)) / sum(x) - (length(x)+1)/length(x) 

Skp1qm1 <- function(k, q) (q-1)/2 * (2*(k+1) + q) + k + 1
all.equal(Skp1qm1(30-1, 70+1), sum(30:100))

# Weighted GINI (by default without correction)
w_gini <- function(x, w, sscor = FALSE) {
  o = radixorder(x)
  w = w[o]
  x = x[o]
  sw = sum(w)
  csw = cumsum(w)
  sx = Skp1qm1(c(0, csw[-length(csw)]), w) 
  if(sscor) return(1 + 2/(sw-1)*(sum(sx*x) / sum(x*w) - sw)) 
  2/sw * sum(sx*x) / sum(x*w) - (sw+1)/sw
}
```

This computes the population-weighted and unweighted GINI on a percentage scale for each year.

```{r}
raw_gini_ts <- nl_pop_data %>% 
  fsubset(pop > 0 & avg_rad > 0) %>%
  fgroup_by(year) %>% 
  fsummarise(gini = gini_noss(avg_rad)*100, 
             w_gini = w_gini(avg_rad, pop)*100) %T>% print()

# Plotting
library(ggplot2)
raw_gini_ts %>% melt(1) %>% 
  ggplot(aes(x = year, y = value, colour = variable)) + 
      geom_line()
```

As evident from the plot, the population-weighted GINI is more smooth, which could be due to unpopulated areas exhibiting greater fluctuations in nightlights (such as fires or flares). 

A final thing that we can do is calibrate the GINI to an official estimate. I use the [*africamonitor*](<https://CRAN.R-project.org/package=africamonitor>) R API to get World Bank GINI estimates for South Africa.

```{r}
WB_GINI <- africamonitor::am_data("ZAF", "SI_POV_GINI") %T>% print()
```

The last estimate in the series is from 2014, estimating a GINI of 63%. To bring the nightlights data in line with this estimate, I again use `optimize()` to determine an appropriate power weight:

```{r}
np_pop_data_pos_14 <- nl_pop_data %>% 
  fsubset(pop > 0 & avg_rad > 0 & year == 2014, year, pop, avg_rad) 

objective <- function(k) {
  nl_gini = np_pop_data_pos_14 %$% w_gini(avg_rad^k, pop) * 100
  abs(63 - nl_gini)
}

res <- optimize(objective, c(0.0001, 5)) %T>% print()
```

With the ideal weight determined, it is easy to obtain a final calibrated nightlights-based GINI series and use it to extend the World Bank estimate. 

```{r}
final_gini_ts <- nl_pop_data %>% 
  fsubset(pop > 0 & avg_rad > 0) %>%
  fgroup_by(year) %>% 
  fsummarise(nl_gini = w_gini(avg_rad^res$minimum, pop)*100) %T>% print()

final_gini_ts %>% 
  merge(WB_GINI %>% fcompute(year = year(Date), wb_gini = SI_POV_GINI), 
        by = "year", all = TRUE) %>% 
  melt("year", na.rm = TRUE) %>% 
  ggplot(aes(x = year, y = value, colour = variable)) + 
      geom_line() + scale_y_continuous(limits = c(50, 70))
```

It should be noted, at this point, that this estimate and the declining trend it shows may be seriously misguided. Research by [Galimberti et al. (2020)](<https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3786572>) using the old [DMSP OLS Nightlights](<https://developers.google.com/earth-engine/datasets/catalog/NOAA_DMSP-OLS_NIGHTTIME_LIGHTS>) series from 1992-2013 for 234 countries and territories, shows that nightlights based inequality measures much better resemble the cross-sectional variation in inequality between countries than the time series dimension within countries. 

The example is nevertheless instrumental in showing how the *fastverse*, in various respects, facilitates and enables complex data science in R. 

# The Future

Future development of *collapse* will see an increased use of SIMD instructions to further increase performance. The impact of such instructions - visible in frameworks like Apache [*arrow*](https://github.com/apache/arrow) and Python's [*polars*](https://github.com/pola-rs/polars) (which is based on *arrow*) can be considerable. The following shows a benchmark computing the means of a matrix with 100 columns and 1 million rows using base R, collapse 1.9.0 (no SIMD), and collapse 1.9.5 (with SIMD).

```{r}
library(collapse)
library(microbenchmark)

fmean19 <- collapsedev19::fmean
m <- rnorm(1e8)
dim(m) <- c(1e6, 100) # matrix with 100 columns and 1 million rows

microbenchmark(colMeans(m), 
               fmean19(m, na.rm = FALSE), 
               fmean(m, na.rm = FALSE), 
               fmean(m), # default is na.rm = TRUE, can be changed with set_collapse()
               fmean19(m, nthreads = 4, na.rm = FALSE), 
               fmean(m, nthreads = 4, na.rm = FALSE), 
               fmean(m, nthreads = 4))
```

Despite these impressive results, I am somewhat doubtful that much of *collapse* will benefit from SIMD. The main reason is that SIMD is a low-level vectorization that can be used to speed up simple operations like addition, subtraction, division, and multiplication. This is especially effective with large amounts of adjacent data. But with many groups and little data in each group, serial programming can be just as efficient or even more efficient if it allows writing grouped operations in a non-nested way. So it depends on the data to groups ratio. My [arrow benchmark](https://github.com/SebKrantz/collapse/blob/master/misc/arrow%20benchmark/arrow_benchmark.md) from August 2022 showed just that: with few groups relative to the data size, *arrow* considerably outperforms *collapse* and *data.table*, but with more groups the latter catch up considerably and *collapse* took lead with many very small groups. More complex statistics algorithms like the median (involving selection) or mode / distinct value count (involving hashing), also cannot (to my knowledge) benefit from SIMD, and here *collapse* implementations are already pretty much state of the art.

Apart from additional vectorization, I am also considering a possible broadening of the package to support further data manipulation operations such as table joins. This may take a while for me to get into though, so I cannot promise an update including this in 2023. At this stage, I am very happy with the API, so no changes are planned here, and I will also try to keep *collapse* harmonious with other *fastverse* packages, in particular *data.table* and *kit*.

Most of all, I hope to see an increased breadth of statistical R packages using *collapse* as a backend, so that its potential for increasing the performance and complexity of statistical R packages is realized in the community. I have in the past assisted package maintainers interested in developing *collapse* backends and hope to increase further collaborations along these lines.

<!--
I also hope that my own discipline of economics would realize the potential of *collapse* and the *fastverse* for economic research. The possibility to apply complex statistical operations effectively to large in-memory datasets - on which most modern day research is still based - should facilitate economic analysis along the lines outlined above. 
-->

At last, I wish to thank all users that provided feedback and inspiration or promoted this software in the community, and more generally all people that encouraged, contributed to, and facilitated these projects. Much credit is also due to the CRAN maintainers who endured many of my mistakes and insisted on high standards, which made *collapse* better and more robust.

```{r, echo=FALSE}
options(oldopts)
```
